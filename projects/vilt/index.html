<!DOCTYPE html><!--hpq7f_F_I4gD8yKufEBGu--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/03fc1b4a8d284b5e-s.p.af4fcd24.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/1e2b70bb24fd69b4.p.870d0e00.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/248e1dc0efc99276-s.p.8a6b2436.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/83afe278b6a6bb3c-s.p.3a6ba036.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/8c2eb9ceedecfc8e-s.p.21935807.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/57320222?s=400&amp;u=8ef7930dfe204b945953f92e032622c61974898f&amp;v=4"/><link rel="preload" as="image" href="https://github.com/imrnh/vilt/raw/main/bin/presentation/demo_1.75x.gif"/><link rel="preload" as="image" href="https://github.com/imrnh/vilt/raw/main/bin/presentation/1.png"/><link rel="preload" as="image" href="https://github.com/imrnh/vilt/raw/main/bin/presentation/2.png"/><link rel="preload" as="image" href="https://github.com/imrnh/vilt/raw/main/model/data/architecture_overview.png"/><link rel="stylesheet" href="/_next/static/chunks/638b318308662e2f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/8cb9b4f8c65c6fd6.js"/><script src="/_next/static/chunks/29bdd735087d69e6.js" async=""></script><script src="/_next/static/chunks/090fffd49a7d5a42.js" async=""></script><script src="/_next/static/chunks/a374d1e1eba94a62.js" async=""></script><script src="/_next/static/chunks/turbopack-c158e9ad20b4203f.js" async=""></script><script src="/_next/static/chunks/1c8935cb71d5ebcf.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/247eb132b7f7b574.js" async=""></script><script src="/_next/static/chunks/71168b994b248471.js" async=""></script><meta name="next-size-adjust" content=""/><title>Imran Hossen | Portfolio</title><meta name="description" content="Developer &amp; Designer Portfolio"/><link rel="icon" href="/favicon.ico?favicon.630925ba.ico" sizes="115x115" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable ibm_plex_sans_d383544b-module__P3SOZq__variable lora_1ae38320-module__0WfQHW__variable inter_fe8b9d92-module__LINzvG__variable merriweather_620cbed2-module__e3WnIq__variable cascadia_mono_a8beebe6-module__cn05aq__variable antialiased bg-white text-gray-900 min-h-screen font-sans"><div hidden=""><!--$--><!--/$--></div><div class="max-w-6xl mx-auto px-6"><nav class="flex flex-wrap items-center justify-between w-full py-8"><a href="/"><div class="flex items-center gap-3 order-1"><img src="https://avatars.githubusercontent.com/u/57320222?s=400&amp;u=8ef7930dfe204b945953f92e032622c61974898f&amp;v=4" alt="Imran Hossen" class="w-10 h-10 rounded-full border-0 border-gray-300"/><span class="font-medium text-lg text-gray-900" style="font-family:var(--font-inter)">Imran Hossen</span></div></a><div class="w-full md:w-auto order-3 md:order-2 flex items-center justify-center gap-6 md:gap-8 mt-4 md:mt-0"><a class="font-mono text-sm text-gray-700 hover:text-gray-900 transition-colors" href="/projects/">01 projects</a><a class="font-mono text-sm text-gray-700 hover:text-gray-900 transition-colors" href="/papers/">02 research</a><a class="font-mono text-sm text-gray-700 hover:text-gray-900 transition-colors" href="/blogs/">03 blog</a></div><div class="flex items-center gap-2 order-2 md:order-3"><a href="https://github.com/imrnh" target="_blank" rel="noopener noreferrer" aria-label="GitHub" class="p-2 text-black hover:text-gray-900 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div></nav><main><article><h1 class="text-4xl font-bold text-gray-900 mb-6 mt-8">Virtual try on application</h1>
<p class="text-gray-700 mb-4 leading-7">Mutual Self-attention based <strong class="font-bold text-gray-900">Dual U-Net</strong> architecture for virtual try on. Please visit https://vilt.vercel.app to try it out.</p>
<center>
<img class="rounded-lg shadow-lg max-w-full h-auto my-6" src="https://github.com/imrnh/vilt/raw/main/bin/presentation/demo_1.75x.gif" alt="Project Demo" width="60%"/>
<p class="text-gray-700 mb-4 leading-7">or watch the video here: https://youtu.be/izVLqsLxXvg</p>
<br/>
<br/>
<table><tbody><tr><td>
<br/>
<img class="rounded-lg shadow-lg max-w-full h-auto my-6" src="https://github.com/imrnh/vilt/raw/main/bin/presentation/1.png"/>
</td><td>
<br/>
<img class="rounded-lg shadow-lg max-w-full h-auto my-6" src="https://github.com/imrnh/vilt/raw/main/bin/presentation/2.png"/>
</td></tr></tbody></table>
</center>
<br/>
<br/>
<h1 class="text-4xl font-bold text-gray-900 mb-6 mt-8">Model Architecture Overview</h1>
<p class="text-gray-700 mb-4 leading-7">The system is a <strong class="font-bold text-gray-900">two-stage, dual-U-Net diffusion model</strong>. It is designed to &quot;try on&quot; a garment from a source image <em class="italic">(cloth_image)</em> onto a target person from another image <em class="italic">(person_image)</em>.</p>
<p class="text-gray-700 mb-4 leading-7">Unlike standard diffusion models that use a single U-Net, this architecture employs two distinct U-Nets:</p>
<ol class="list-decimal ml-6 mb-4 text-gray-700">
<li class="mb-2">
<p class="text-gray-700 mb-4 leading-7"><strong class="font-bold text-gray-900">Cloth U-Net:</strong> Its sole purpose is to process the garment image and extract its key visual features (like texture, pattern, and color).</p>
</li>
<li class="mb-2">
<p class="text-gray-700 mb-4 leading-7"><strong class="font-bold text-gray-900">Denoising U-Net:</strong> This is the main image generator. It builds the new image of the person wearing the garment by progressively denoising a random latent. It is guided by two sources: the encoded person image and the features from the Reference U-Net.</p>
</li>
</ol>
<p class="text-gray-700 mb-4 leading-7">These two U-Nets are coupled to inject the garment&#x27;s features into the denoising process using mutual self-attention.</p>
<center><br/>
<img class="rounded-lg shadow-lg max-w-full h-auto my-6" src="https://github.com/imrnh/vilt/raw/main/model/data/architecture_overview.png"/>
<br/>
<p class="text-gray-700 mb-4 leading-7"><em class="italic">Fig: Mutual Self-Attention Mechanism of Cloth and Person U-Net models.</em></p>
</center>
<h2 class="text-3xl font-bold text-gray-900 mb-4 mt-6">ðŸ”‘ Key Mechanism (Mutual Self-Attention)</h2>
<p class="text-gray-700 mb-4 leading-7">Intercepting the self-attention blocks of both U-Nets to pass information. This operation is done in two steps:</p>
<p class="text-gray-700 mb-4 leading-7"><strong class="font-bold text-gray-900">Collecting Cloth Features:</strong> When the <strong class="font-bold text-gray-900">cloth_unet</strong> performs its single forward pass on the <strong class="font-bold text-gray-900">cloth_image_latent</strong>, this controller activates. At each transformer block, the controller intercepts the input features to the attention layer. After the complete pass, it contains a multi-scale &quot;fingerprint&quot; of the garment, capturing its features at all levels of the U-Net.</p>
<p class="text-gray-700 mb-4 leading-7"><strong class="font-bold text-gray-900">Injecting Cloth Feature:</strong> Before the denoising loop begins, the system hand over all the cloth feature to the denoising U-Net. During the denoising loop, as the <strong class="font-bold text-gray-900">Denoising U-Net</strong> processes its own latents (representing the person), its modified self-attention blocks (<strong class="font-bold text-gray-900">BasicTransformerBlock_Denoising</strong>) execute the following:</p>
<ol class="list-decimal ml-6 mb-4 text-gray-700">
<li class="mb-2">The block&#x27;s <strong class="font-bold text-gray-900">Query (Q)</strong> is computed from its own hidden states (the person features).</li>
<li class="mb-2">The block retrieves the corresponding garment features for that layer.</li>
<li class="mb-2">It constructs a new <strong class="font-bold text-gray-900">Key (K)</strong> and <strong class="font-bold text-gray-900">Value (V)</strong> by <strong class="font-bold text-gray-900">concatenating</strong> its <em class="italic">own</em> hidden states with the <em class="italic">garment features</em> for that layer.</li>
<li class="mb-2">The attention mechanism then computes: <strong class="font-bold text-gray-900">Attention(Q_person, K_[person+garment], V_[person+garment])</strong>.</li>
</ol>
<p class="text-gray-700 mb-4 leading-7">This <strong class="font-bold text-gray-900">Mutual Self-Attention</strong> forces the denoising network (while generating the person) to look up and pull relevant visual information from the garment features at every layer, effectively &quot;draping&quot; the garment&#x27;s texture and style onto the generated person&#x27;s shape.</p>
<h2 class="text-3xl font-bold text-gray-900 mb-4 mt-6">Core Components</h2>
<ul class="list-disc ml-6 mb-4 text-gray-700">
<li class="mb-2">
<p class="text-gray-700 mb-4 leading-7"><strong class="font-bold text-gray-900">VAE:</strong> A standard Variational Autoencoder used to embed images.</p>
</li>
<li class="mb-2">
<p class="text-gray-700 mb-4 leading-7"><strong class="font-bold text-gray-900">Cloth U-Net:</strong> Its sole purpose is to process the VAE-encoded garment latent and extract a set of multi-scale visual features (texture, pattern, color, etc.).</p>
</li>
<li class="mb-2">
<p class="text-gray-700 mb-4 leading-7"><strong class="font-bold text-gray-900">Denoising U-Net:</strong> This is the primary generator U-Net. It synthesizes the final image by progressively denoising a random latent. Its operation is guided by two <strong class="font-bold text-gray-900">person image latent</strong>  and the features injected from the Cloth U-Net.</p>
</li>
</ul></article><!--$--><!--/$--></main></div><script src="/_next/static/chunks/8cb9b4f8c65c6fd6.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[55672,[\"/_next/static/chunks/1c8935cb71d5ebcf.js\"],\"default\"]\n3:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n4:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n6:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\n7:\"$Sreact.suspense\"\n9:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\nb:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\nd:I[68027,[],\"default\"]\n:HL[\"/_next/static/chunks/638b318308662e2f.css\",\"style\"]\n:HL[\"/_next/static/media/03fc1b4a8d284b5e-s.p.af4fcd24.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/1e2b70bb24fd69b4.p.870d0e00.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/248e1dc0efc99276-s.p.8a6b2436.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/83afe278b6a6bb3c-s.p.3a6ba036.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/8c2eb9ceedecfc8e-s.p.21935807.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"hpq7f_F-I4gD8yKufEBGu\",\"c\":[\"\",\"projects\",\"vilt\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"projects\",{\"children\":[[\"slug\",\"vilt\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/638b318308662e2f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/1c8935cb71d5ebcf.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable ibm_plex_sans_d383544b-module__P3SOZq__variable lora_1ae38320-module__0WfQHW__variable inter_fe8b9d92-module__LINzvG__variable merriweather_620cbed2-module__e3WnIq__variable cascadia_mono_a8beebe6-module__cn05aq__variable antialiased bg-white text-gray-900 min-h-screen font-sans\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-6\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/71168b994b248471.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@8\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L9\",null,{\"children\":\"$@a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@c\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[91397,[\"/_next/static/chunks/1c8935cb71d5ebcf.js\",\"/_next/static/chunks/71168b994b248471.js\"],\"default\"]\nf:Te58,"])</script><script>self.__next_f.push([1,"# Virtual try on application\n\nMutual Self-attention based **Dual U-Net** architecture for virtual try on. Please visit https://vilt.vercel.app to try it out. \n\n\u003ccenter\u003e\n\n\u003cimg src=\"https://github.com/imrnh/vilt/raw/main/bin/presentation/demo_1.75x.gif\" alt=\"Project Demo\" width=\"60%\"\u003e\n\nor watch the video here: https://youtu.be/izVLqsLxXvg\n\n\u003cbr\u003e\n\u003cbr\u003e\n\n\n\u003ctable\u003e\n\u003ctd\u003e\n\u003cbr\u003e\n\u003cimg src=\"https://github.com/imrnh/vilt/raw/main/bin/presentation/1.png\"\u003e\n\u003c/td\u003e\n\u003ctd\u003e\n\u003cbr\u003e\n\u003cimg src=\"https://github.com/imrnh/vilt/raw/main/bin/presentation/2.png\"\u003e\n\u003c/td\u003e\n\u003c/table\u003e\n\n\u003c/center\u003e\n\n\n\u003cbr\u003e\n\u003cbr\u003e\n\n\n\n# Model Architecture Overview\n\nThe system is a **two-stage, dual-U-Net diffusion model**. It is designed to \"try on\" a garment from a source image *(cloth_image)* onto a target person from another image *(person_image)*.\n\n\nUnlike standard diffusion models that use a single U-Net, this architecture employs two distinct U-Nets:\n\n1. **Cloth U-Net:** Its sole purpose is to process the garment image and extract its key visual features (like texture, pattern, and color).\n\n2. **Denoising U-Net:** This is the main image generator. It builds the new image of the person wearing the garment by progressively denoising a random latent. It is guided by two sources: the encoded person image and the features from the Reference U-Net.\n\n\nThese two U-Nets are coupled to inject the garment's features into the denoising process using mutual self-attention.\n\n\n\u003ccenter\u003e\u003cbr\u003e\n\u003cimg src=\"https://github.com/imrnh/vilt/raw/main/model/data/architecture_overview.png\"\u003e\n\u003cbr\u003e\n\n*Fig: Mutual Self-Attention Mechanism of Cloth and Person U-Net models.*\n\n\u003c/center\u003e\n\n\n\n\n\n## ðŸ”‘ Key Mechanism (Mutual Self-Attention)\n\nIntercepting the self-attention blocks of both U-Nets to pass information. This operation is done in two steps:\n\n**Collecting Cloth Features:** When the **cloth_unet** performs its single forward pass on the **cloth_image_latent**, this controller activates. At each transformer block, the controller intercepts the input features to the attention layer. After the complete pass, it contains a multi-scale \"fingerprint\" of the garment, capturing its features at all levels of the U-Net.\n\n**Injecting Cloth Feature:** Before the denoising loop begins, the system hand over all the cloth feature to the denoising U-Net. During the denoising loop, as the **Denoising U-Net** processes its own latents (representing the person), its modified self-attention blocks (**BasicTransformerBlock_Denoising**) execute the following:\n1.  The block's **Query (Q)** is computed from its own hidden states (the person features).\n2.  The block retrieves the corresponding garment features for that layer.\n3.  It constructs a new **Key (K)** and **Value (V)** by **concatenating** its *own* hidden states with the *garment features* for that layer.\n4.  The attention mechanism then computes: **Attention(Q_person, K_[person+garment], V_[person+garment])**.\n\nThis **Mutual Self-Attention** forces the denoising network (while generating the person) to look up and pull relevant visual information from the garment features at every layer, effectively \"draping\" the garment's texture and style onto the generated person's shape.\n\n\n## Core Components\n\n* **VAE:** A standard Variational Autoencoder used to embed images.\n\n* **Cloth U-Net:** Its sole purpose is to process the VAE-encoded garment latent and extract a set of multi-scale visual features (texture, pattern, color, etc.).\n\n* **Denoising U-Net:** This is the primary generator U-Net. It synthesizes the final image by progressively denoising a random latent. Its operation is guided by two **person image latent**  and the features injected from the Cloth U-Net.\n\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$Le\",null,{\"content\":\"$f\"}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"10:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"IconMark\"]\nc:[[\"$\",\"title\",\"0\",{\"children\":\"Imran Hossen | Portfolio\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Developer \u0026 Designer Portfolio\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.630925ba.ico\",\"sizes\":\"115x115\",\"type\":\"image/x-icon\"}],[\"$\",\"$L10\",\"3\",{}]]\n8:null\n"])</script></body></html>