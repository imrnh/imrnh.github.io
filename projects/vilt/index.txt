1:"$Sreact.fragment"
2:I[55672,["/_next/static/chunks/1c8935cb71d5ebcf.js"],"default"]
3:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"default"]
4:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"default"]
6:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"OutletBoundary"]
7:"$Sreact.suspense"
9:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"ViewportBoundary"]
b:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"MetadataBoundary"]
d:I[68027,[],"default"]
:HL["/_next/static/chunks/638b318308662e2f.css","style"]
:HL["/_next/static/media/03fc1b4a8d284b5e-s.p.af4fcd24.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/1e2b70bb24fd69b4.p.870d0e00.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/248e1dc0efc99276-s.p.8a6b2436.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/83afe278b6a6bb3c-s.p.3a6ba036.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/8c2eb9ceedecfc8e-s.p.21935807.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"hpq7f_F-I4gD8yKufEBGu","c":["","projects","vilt",""],"q":"","i":false,"f":[[["",{"children":["projects",{"children":[["slug","vilt","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/638b318308662e2f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/1c8935cb71d5ebcf.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable ibm_plex_sans_d383544b-module__P3SOZq__variable lora_1ae38320-module__0WfQHW__variable inter_fe8b9d92-module__LINzvG__variable merriweather_620cbed2-module__e3WnIq__variable cascadia_mono_a8beebe6-module__cn05aq__variable antialiased bg-white text-gray-900 min-h-screen font-sans","children":["$","div",null,{"className":"max-w-6xl mx-auto px-6","children":[["$","$L2",null,{}],["$","main",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L5",[["$","script","script-0",{"src":"/_next/static/chunks/71168b994b248471.js","async":true,"nonce":"$undefined"}]],["$","$L6",null,{"children":["$","$7",null,{"name":"Next.MetadataOutlet","children":"$@8"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$L9",null,{"children":"$@a"}],["$","div",null,{"hidden":true,"children":["$","$Lb",null,{"children":["$","$7",null,{"name":"Next.Metadata","children":"$@c"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$d",[]],"s":false,"S":true}
e:I[91397,["/_next/static/chunks/1c8935cb71d5ebcf.js","/_next/static/chunks/71168b994b248471.js"],"default"]
f:Te58,# Virtual try on application

Mutual Self-attention based **Dual U-Net** architecture for virtual try on. Please visit https://vilt.vercel.app to try it out. 

<center>

<img src="https://github.com/imrnh/vilt/raw/main/bin/presentation/demo_1.75x.gif" alt="Project Demo" width="60%">

or watch the video here: https://youtu.be/izVLqsLxXvg

<br>
<br>


<table>
<td>
<br>
<img src="https://github.com/imrnh/vilt/raw/main/bin/presentation/1.png">
</td>
<td>
<br>
<img src="https://github.com/imrnh/vilt/raw/main/bin/presentation/2.png">
</td>
</table>

</center>


<br>
<br>



# Model Architecture Overview

The system is a **two-stage, dual-U-Net diffusion model**. It is designed to "try on" a garment from a source image *(cloth_image)* onto a target person from another image *(person_image)*.


Unlike standard diffusion models that use a single U-Net, this architecture employs two distinct U-Nets:

1. **Cloth U-Net:** Its sole purpose is to process the garment image and extract its key visual features (like texture, pattern, and color).

2. **Denoising U-Net:** This is the main image generator. It builds the new image of the person wearing the garment by progressively denoising a random latent. It is guided by two sources: the encoded person image and the features from the Reference U-Net.


These two U-Nets are coupled to inject the garment's features into the denoising process using mutual self-attention.


<center><br>
<img src="https://github.com/imrnh/vilt/raw/main/model/data/architecture_overview.png">
<br>

*Fig: Mutual Self-Attention Mechanism of Cloth and Person U-Net models.*

</center>





## ðŸ”‘ Key Mechanism (Mutual Self-Attention)

Intercepting the self-attention blocks of both U-Nets to pass information. This operation is done in two steps:

**Collecting Cloth Features:** When the **cloth_unet** performs its single forward pass on the **cloth_image_latent**, this controller activates. At each transformer block, the controller intercepts the input features to the attention layer. After the complete pass, it contains a multi-scale "fingerprint" of the garment, capturing its features at all levels of the U-Net.

**Injecting Cloth Feature:** Before the denoising loop begins, the system hand over all the cloth feature to the denoising U-Net. During the denoising loop, as the **Denoising U-Net** processes its own latents (representing the person), its modified self-attention blocks (**BasicTransformerBlock_Denoising**) execute the following:
1.  The block's **Query (Q)** is computed from its own hidden states (the person features).
2.  The block retrieves the corresponding garment features for that layer.
3.  It constructs a new **Key (K)** and **Value (V)** by **concatenating** its *own* hidden states with the *garment features* for that layer.
4.  The attention mechanism then computes: **Attention(Q_person, K_[person+garment], V_[person+garment])**.

This **Mutual Self-Attention** forces the denoising network (while generating the person) to look up and pull relevant visual information from the garment features at every layer, effectively "draping" the garment's texture and style onto the generated person's shape.


## Core Components

* **VAE:** A standard Variational Autoencoder used to embed images.

* **Cloth U-Net:** Its sole purpose is to process the VAE-encoded garment latent and extract a set of multi-scale visual features (texture, pattern, color, etc.).

* **Denoising U-Net:** This is the primary generator U-Net. It synthesizes the final image by progressively denoising a random latent. Its operation is guided by two **person image latent**  and the features injected from the Cloth U-Net.

5:["$","$Le",null,{"content":"$f"}]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
10:I[27201,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"IconMark"]
c:[["$","title","0",{"children":"Imran Hossen | Portfolio"}],["$","meta","1",{"name":"description","content":"Developer & Designer Portfolio"}],["$","link","2",{"rel":"icon","href":"/favicon.ico?favicon.630925ba.ico","sizes":"115x115","type":"image/x-icon"}],["$","$L10","3",{}]]
8:null
